
###################################################
################ concurrent ########################
####################################################
def crawlpage(page_idx: int = 1, parameter: str = "computer science"):
    baseurl = "https://pubmed.ncbi.nlm.nih.gov/"
    url=""
    header={"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32"}
    result_all = -1

    findlink_PMID = re.compile(r'<span class="citation-part".*>(\d+)<\/span>')  # 提取PMID号，作为下载的地址
    findlink_journal = re.compile(r'journal-citation">(.*?) doi:.*?\.[ <]')  # 查找期刊名称
    findlink_doi = re.compile(r'journal-citation">.*? (doi:.*?\.)[ <]')  # 查找文献doi
    findlink_title = re.compile(r'<a class="docsum-title".*?">.*?([A-Z0-9].*?)[.?]', re.S) # 提取文章的title，含有</b>符号。每页有50个文章的title
    findlink_auto_info = re.compile(r'full-authors">(.*?)<\/span>')  # 查找作者名称信息
    findlink_FREE_PMC_MARK = re.compile(r'<span class="free-resources.*?>(.*)\.<')  # 提取free pmc article标志
    findlink_review = re.compile(r'citation-part">Review.<\/span>')  # 查找文献review标签

    url=baseurl+"?"+ parameter + "&page=" + str(page_idx)
    request=urllib.request.Request(url,headers=header)
    response=urllib.request.urlopen(request)
    html=response.read()
    content = BeautifulSoup(html, "html.parser")
    data = []
    try:
        for item in content.find_all("div", class_="docsum-content"):
            ''' 获取 PMID, 作者, 期刊, doi, title, 是否free, 是否 review '''
            item = str(item)
            
            # 获取 PMID
            PMID = re.findall(findlink_PMID, item)[0]

            # 获取作者
            authorlist = re.search(findlink_auto_info, item)
            if authorlist != None:
                authorlist = authorlist.group(1)
            else:
                authorlist = ''

            # 获取期刊
            journal = re.search(findlink_journal, item)  
            if journal == None:
                journal = ''
            else:
                journal = journal.group(1)


            # 获取 doi
            doi = re.search(findlink_doi, item)  # 少数文献是没有doi号的，直接用item[0]会导致index超出
            if doi == None:
                doi = ''
            else:
                doi = doi.group(1)

            # 获取 title
            doctitle = re.findall(findlink_title, item)[0]
            doctitle = doctitle.replace("-/-", '')
            doctitle = re.sub(r"<.+?>", '', doctitle)

            # freemark flag: 0，不是免费文件无原文，1 是free article无原文， 2 是有pmc原文
            freemark = re.findall(findlink_FREE_PMC_MARK, item)
            if len(freemark) == 0:
                freemark = '0'
            else:
                if len(freemark[0]) == 16:
                    freemark = '2'
                else:
                    freemark = '1'

            # 1 表示文章类型是 review
            reviewmark = re.search(findlink_review, item)
            # print(reviewmark.group())
            if reviewmark == None:
                reviewmark = '0'
            else:
                reviewmark = '1'

            temp = []
            temp.append(doctitle)
            temp.append(authorlist)
            temp.append(journal)
            temp.append(doi)
            temp.append(PMID)
            temp.append(freemark)
            temp.append(reviewmark)
            data.append(temp)
    except Exception as e:
        print(e)
        return None
        
    return data


def download(item):
    try:
        pdfstream = download_pubmed_pdf_using_paper_metadata(item)
        savepath = f"./document/neurosurgery/{item[4]}.pdf"  
        file = open(savepath, 'wb')
        file.write(pdfstream)
        file.close()
        print('success')
    except Exception as e:
        # 将异常信息写入日志文件
        logging.error(f"Error: {e}, PMID: {item[4]}")


def fetch_and_download(page_idx):
    items = crawlpage(parameter = args.keywords, page_idx=page_idx)
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(download, items)

def main(startpage, endpage):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(fetch_and_download, range(startpage, endpage))


if __name__ == "__main__":
    startpage = 1  # 根据需要设置
    endpage = args.endpage   # 根据需要设置
    main(startpage, endpage)

