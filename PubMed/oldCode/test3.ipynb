{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'brain surgery'\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from tqdm import tqdm\n",
    "\n",
    "from timevar import savetime\n",
    "import time \n",
    "import random\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "import urllib\n",
    "import urllib.error\n",
    "from urllib import request\n",
    "\n",
    "import eventlet\n",
    "import xlwt\n",
    "\n",
    "from geteachinfo import readdata1\n",
    "from timevar import savetime\n",
    "import glob\n",
    "\n",
    "file_list = glob.glob('./document/neurosurgery/*.pdf')\n",
    "number_list = []\n",
    "\n",
    "# Loop through the original list and extract the numbers\n",
    "for file in file_list:\n",
    "    # Split the string by '/' and then by '.' to isolate the number\n",
    "    number = file.split('/')[-1].split('.')[0]\n",
    "    number_list.append(number)\n",
    "\n",
    "\n",
    "class Search_param(str):\n",
    "    \"\"\"\n",
    "    搜索参数管理类，用于拼接在baseurl之后\n",
    "\n",
    "    可以由generate方法encode生成初次检索所需的URL后缀\n",
    "    例如：”?term=alzheimer%27s+disease“\n",
    "    可以附加一些调整网页的属性\n",
    "\n",
    "    TODO: 按日期搜索，\n",
    "    2020.1.1 - 2020.1.31: \n",
    "    https://pubmed.ncbi.nlm.nih.gov/?term=computer+science&filter=dates.2020%2F1%2F1-2020%2F1%2F31\n",
    "\n",
    "    2024.5.1 - 2024.5.31\n",
    "    https://pubmed.ncbi.nlm.nih.gov/?term=computer+science&filter=dates.2024%2F5%2F1-2024%2F5%2F31\n",
    "    \"\"\"\n",
    "    def __init__(self, keywords:str):\n",
    "        self.search_keywords = {}\n",
    "        self.search_keywords['term'] = keywords.strip()\n",
    "\n",
    "    def gen_search_param(self) -> str:\n",
    "        # encode url生成request需要的url\n",
    "        return urllib.parse.urlencode(self.search_keywords)\n",
    "\n",
    "    def specify_web_size(self, size: int):\n",
    "        # 调整 搜索页面的大小\n",
    "        self.search_keywords['size'] = size\n",
    "\n",
    "    def specify_any_param(self, key: str, value):\n",
    "        \"\"\"\n",
    "        针对任意参数进行调整, 需要提供合适的键值对，默认不存在\n",
    "        目前观察到的有：sort(date, pubdate, fauth, jour), sort_order(asc) 更多参数请查看pubmed的搜索URL\n",
    "        :param key: url链接需要添加的键\n",
    "        :param value: url链接中键对应的值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.search_keywords[key] = value\n",
    "\n",
    "def spider_pubmed_paper_metadata(parameter: str = \"computer science\", startpage: int = 1,  endpage: int = 12):\n",
    "    \"\"\" return: metadata of papers\n",
    "        爬虫函数主体 \n",
    "        startpage 是从搜索结果第几页起爬\n",
    "        endpage 是搜索结果第几页结束爬\n",
    "        返回： datalist: list = [] 每个元素是一篇论文的情况\n",
    "        每个元素的内容： \n",
    "        ```\n",
    "        0: document title\n",
    "        1: author list\n",
    "        2: journal \n",
    "        3: doi \n",
    "        4: PMID\n",
    "        5: freemark, 是否free, 2 是有原文的\n",
    "        6: reviewmark, 是否 review\n",
    "        ```\n",
    "    \"\"\"\n",
    "    datalist = [] # startpage 到 endpage 的页中的论文列表\n",
    "\n",
    "\n",
    "    baseurl = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "    url=\"\"\n",
    "    header={\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32\"}\n",
    "\n",
    "    # 先获取所有搜索结果：\n",
    "    result_all = -1\n",
    "\n",
    "    # 下列正则表达式用来搜索\n",
    "    findlink_PMID = re.compile(r'<span class=\"citation-part\".*>(\\d+)<\\/span>')  # 提取PMID号，作为下载的地址\n",
    "    findlink_journal = re.compile(r'journal-citation\">(.*?) doi:.*?\\.[ <]')  # 查找期刊名称\n",
    "    findlink_doi = re.compile(r'journal-citation\">.*? (doi:.*?\\.)[ <]')  # 查找文献doi\n",
    "    findlink_title = re.compile(r'<a class=\"docsum-title\".*?\">.*?([A-Z0-9].*?)[.?]', re.S) # 提取文章的title，含有</b>符号。每页有50个文章的title\n",
    "    findlink_auto_info = re.compile(r'full-authors\">(.*?)<\\/span>')  # 查找作者名称信息\n",
    "    findlink_FREE_PMC_MARK = re.compile(r'<span class=\"free-resources.*?>(.*)\\.<')  # 提取free pmc article标志\n",
    "    findlink_review = re.compile(r'citation-part\">Review.<\\/span>')  # 查找文献review标签\n",
    "\n",
    "    progress_bar = tqdm(total=endpage - startpage)\n",
    "    for i in range(startpage, endpage):\n",
    "\n",
    "        # [1] 开始遍历每一页结果，一共 page 页最大pagemax页\n",
    "        \n",
    "\n",
    "        # [2] 用指定的参数， get 的方式获取 pubmed 的搜索结果 \n",
    "        url=baseurl+\"?\"+ parameter + \"&page=\" + str(i)\n",
    "        request=urllib.request.Request(url,headers=header)\n",
    "        response=urllib.request.urlopen(request)\n",
    "        # print(f'\\033[1;35m url is {url}\\033[0m')\n",
    "        html=response.read()\n",
    "\n",
    "        content = BeautifulSoup(html, \"html.parser\")\n",
    "        # [3] 计算 result_all 是 <span class=\"value\">498,730</span> 中的 498730。网页展示 498,730 results\n",
    "        result_all = int(str(content.find_all(\"span\", class_=\"value\")[0])[20:-7].replace(',', '')) \n",
    "        \n",
    "\n",
    "        # [3] 把一个页面的 50 条结果全部返回成一个 list\n",
    "        #data = traverse(html)\n",
    "        data = []\n",
    "        # print(f'\\033[1;35m len of item is {len(content.find_all(\"div\", class_=\"docsum-content\"))}\\033[0m')\n",
    "        for item in content.find_all(\"div\", class_=\"docsum-content\"):\n",
    "            ''' 获取 PMID, 作者, 期刊, doi, title, 是否free, 是否 review '''\n",
    "            item = str(item)\n",
    "            \n",
    "            # 获取 PMID\n",
    "            PMID = re.findall(findlink_PMID, item)[0]\n",
    "\n",
    "            # 获取作者\n",
    "            authorlist = re.search(findlink_auto_info, item)\n",
    "            if authorlist != None:\n",
    "                authorlist = authorlist.group(1)\n",
    "            else:\n",
    "                authorlist = ''\n",
    "\n",
    "            # 获取期刊\n",
    "            journal = re.search(findlink_journal, item)  \n",
    "            if journal == None:\n",
    "                journal = ''\n",
    "            else:\n",
    "                journal = journal.group(1)\n",
    "\n",
    "\n",
    "            # 获取 doi\n",
    "            doi = re.search(findlink_doi, item)  # 少数文献是没有doi号的，直接用item[0]会导致index超出\n",
    "            if doi == None:\n",
    "                doi = ''\n",
    "            else:\n",
    "                doi = doi.group(1)\n",
    "\n",
    "            # 获取 title\n",
    "            doctitle = re.findall(findlink_title, item)[0]\n",
    "            doctitle = doctitle.replace(\"-/-\", '')\n",
    "            doctitle = re.sub(r\"<.+?>\", '', doctitle)\n",
    "\n",
    "            # freemark flag: 0，不是免费文件无原文，1 是free article无原文， 2 是有pmc原文\n",
    "            freemark = re.findall(findlink_FREE_PMC_MARK, item)\n",
    "            if len(freemark) == 0:\n",
    "                freemark = '0'\n",
    "            else:\n",
    "                if len(freemark[0]) == 16:\n",
    "                    freemark = '2'\n",
    "                else:\n",
    "                    freemark = '1'\n",
    "\n",
    "            # 1 表示文章类型是 review\n",
    "            reviewmark = re.search(findlink_review, item)\n",
    "            # print(reviewmark.group())\n",
    "            if reviewmark == None:\n",
    "                reviewmark = '0'\n",
    "            else:\n",
    "                reviewmark = '1'\n",
    "\n",
    "            temp = []\n",
    "            temp.append(doctitle)\n",
    "            temp.append(authorlist)\n",
    "            temp.append(journal)\n",
    "            temp.append(doi)\n",
    "            temp.append(PMID)\n",
    "            temp.append(freemark)\n",
    "            temp.append(reviewmark)\n",
    "            data.append(temp)\n",
    "\n",
    "        progress_bar.update(1)  # 更新进度条\n",
    "        datalist.extend(data) # 直接增加到datalist里， 每个元素就是一篇论文的情况\n",
    "        sleep(random.randint(0, 2))\n",
    "    progress_bar.close()\n",
    "    return datalist\n",
    "\n",
    "def download_pubmed_pdf_using_paper_metadata(metadata):\n",
    "    # metadata 中没有 PMCID，只有PMID。下面获取PMCID\n",
    "    baseurl = \"https://pubmed.ncbi.nlm.nih.gov/\"  # baseurl和之前的搜索页面一致\n",
    "    PMID = str(metadata[4])\n",
    "\n",
    "    # 检查 PMID 是否已存在：\n",
    "    file_list = glob.glob('./document/neurosurgery/*.pdf')\n",
    "    number_list = []\n",
    "\n",
    "    # Loop through the original list and extract the numbers\n",
    "    for file in file_list:\n",
    "        # Split the string by '/' and then by '.' to isolate the number\n",
    "        number = file.split('/')[-1].split('.')[0]\n",
    "        number_list.append(number)\n",
    "        if PMID in number_list:\n",
    "            return None\n",
    "\n",
    "    url = baseurl + PMID\n",
    "    header = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32\"}\n",
    "    request = urllib.request.Request(url, headers=header)\n",
    "    html = \"\"\n",
    "    response = urllib.request.urlopen(request)\n",
    "    html = response.read()\n",
    "    content = BeautifulSoup(html, \"html.parser\")\n",
    "    heading = content.find_all('div', class_=\"full-view\", id=\"full-view-heading\")\n",
    "    heading = str(heading)  # heading是一个包含文章大部分信息的标签\n",
    "    PMCID = re.search(r'(PMC\\d+)\\n', heading)  # 获取PMCID用于后续的自动下载\n",
    "    PMCID = PMCID.group(1)\n",
    "    # 这一页包括了PMCID和全文等其他部分。但在这里，只需要PMCID就能下载到PDF\n",
    "\n",
    "    # 尝试直接从 pubmed 下载\n",
    "\n",
    "    '''\n",
    "    \n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8022506/pdf/peerj-cs-07-441.pdf\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8022506/pdf\n",
    "\n",
    "    29702967\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29702967/\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC36091982/pdf\n",
    "\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7319935/pdf/main.pdf 打得开, PMID 是 32599201\n",
    "\n",
    "    PMID 36091982， 题目，Automatic computer science domain multiple-choice questions generation based on informative sentences\n",
    "    PMCID：\n",
    "    '''\n",
    "\n",
    "    downpara = \"pmc/articles/\" + PMCID + \"/pdf\"\n",
    "    baseurl = \"https://www.ncbi.nlm.nih.gov/\"\n",
    "    url = baseurl + downpara\n",
    "    print(f\"\\033[1;35m[I] pdf url: {url}\\033[0m\", end = \".\")\n",
    "    header={\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.41 Safari/537.36 Edg/101.0.1210.32\"}\n",
    "    request=urllib.request.Request(url,headers=header)\n",
    "    html=\"\"\n",
    "    response = urllib.request.urlopen(request, timeout=60)\n",
    "    html = response.read()\n",
    "    # print(\"%s.pdf\" % metadata[0], \"从目标站获取pdf数据成功\")\n",
    "    return html\n",
    "\n",
    "def fetch_pdf_urls_from_pubmed(keyword = 'computer science', startpage = 1, endpage = 1000):\n",
    "    search_param = Search_param(keyword)\n",
    "    metadatas = spider_pubmed_paper_metadata(search_param.gen_search_param(), startpage = startpage, endpage = endpage)\n",
    "    return metadatas\n",
    "\n",
    "metadatas = fetch_pdf_urls_from_pubmed(keyword = keywords)\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "length = len(metadatas)\n",
    "i = 0\n",
    "for metadata in metadatas:\n",
    "    print(f'{i}/{length}', end = ',')\n",
    "    try:\n",
    "        pdfstream = download_pubmed_pdf_using_paper_metadata(metadata)\n",
    "        if pdfstream != None:\n",
    "            savepath = f\"./document/neurosurgery/{metadata[4]}.pdf\"  # 这里用 PMID 存文件\n",
    "            file = open(savepath, 'wb')\n",
    "            file.write(pdfstream)\n",
    "            file.close()\n",
    "            print('success', end = '\\n')\n",
    "    except Exception as e:\n",
    "        # 将异常信息写入日志文件\n",
    "        logging.error(f\"Error: {e}, PMID: {metadata[4]}\")\n",
    "        print('\\n')\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
